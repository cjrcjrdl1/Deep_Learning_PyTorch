# Deep_Learning_PyTorch
Deep_Learning_PyTorch
Chapter 1 (O)

Chapter 2 (O)

Chapter 3 (O)

비지도학습
```
K-평균 군집화
왜 사용? : 주어진 데이터에 대한 군집화
언제 사용하면 좋을까? : 주어진 데이터셋을 이용해서 몇개의 클러스터를 구성할지 사전에 알 수 있을 때 사용하면 유용
안 좋은 상황 : 데이터가 비선형일 때, 군집 크기가 다를 때, 군집마다 밀집도와 거리가 다를 때
```

```
밀집 기반 군집 분석
왜 사용? : 주어진 데이터에 대한 군집화
언제 사용하면 좋을까? : K-평균 군집화와 다르게 사전에 클러스터의 숫자를 알지 못할 때 사용하면 유용
                      또한, 주어진 데이터에 이상치가 많이 포함되었을 때 사용하면 좋음
노이즈는 주어진 데이터셋과 무관하거나 무작위성 데이터로 전치리 과정에서 제거해야 할 부분
이상치란 관측된 데이터 범위에서 많이 벗어난 아주 작은 값이나 아주 큰 값

```

```
주성분 분석(PCA)
왜 사용? : 주어진 데이터의 간소화
언제 사용하면 좋을까? : 현재 데이터의 특성(변수)이 너무 많은 경우에는 데이터를 하나의 플롯(plot)에 시각화해서 살펴보는 것이 어렵다.
                      이때 특성 p개를 두세 개 정도로 압축하여 데이터를 시각화하여 살펴보고 싶을 때 유용한 알고리즘
변수가 많은 고차원 데이터의 경우 중요하지 않은 변수로 처리해야 할 데이터 양 많아지고 성능 또한 나빠지는 경향이 있다.
이러한 문제를 해결하고자 고차원 데이터를 저차원으로 축소시켜 데이터가 가진 대표 특성만 추출한다면
성능은 좋아지고 작업도 좀 더 간편해짐
대표적인 알고리즘이 PCA -> 고차원 데이터를 저차원 데이터로 축소시키는 알고리즘
```

Chapter 4
```
인공 신경망의 한계 -> AND, OR 는 선형 분류가 잘 되어 학습할 수 있지만,
XOR는 선형 분류를 할 수 없음 -> 해결방안은 중간에 은닉층을 둔 다층 퍼셉트론 사용
입력층과 출력층 사이에 여러 개의 신경망을 심층 신경망(DNN)이라고 하며 심층 신경망을 다른 말로 딥러닝이라고 함
```
```
딥러닝의 층은 입력층, 은닉층, 출력층으로 구성
가중치 : 노드와 노드 간 연결 강도
바이어스 : 가중치에 더해 주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값 조절
가중합 : 가중치와 신호의 곱을 합한 것
활성화 함수 : 신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수
손실 함수 : 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수
```
```
활성화 함수
전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수
시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 있음

시그모이드 함수
선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해 준다.
주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용된다.
하지만 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 '기울기 소멸 문제'가 발생하여 딥러닝에서 잘 사용X

하이퍼볼릭 탄젠트 함수
선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형해 준다.
시그모이드에서 결과 값 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만,
기울기 소멸 문제가 여전히 발생

렐루 함수
입력(x)이 음수일 때는 0을 양수일 때는 x를 출력
경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점이 있음
렐루 함수는 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수보다 6배 빠름
문제는 음수 값을 받으면 항상 0을 출력하기 때문에 학습 능력이 감소하는데,
이를 해결하려고 리키 렐루 함수 등을 사용

리키 렐루 함수
입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환
이렇게 하면 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제 해결

소프트맥스 함수
입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 한다.
소프트맥스 함수는 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용된다.
exp(x)는 지수함수이다. n은 출력층의 뉴런 개수, yk는 그중 k번째 출력을 의미
```
