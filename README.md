# Deep_Learning_PyTorch
Deep_Learning_PyTorch
Chapter 1 (O)

Chapter 2 (O)

Chapter 3 (O)

비지도학습
```
K-평균 군집화
왜 사용? : 주어진 데이터에 대한 군집화
언제 사용하면 좋을까? : 주어진 데이터셋을 이용해서 몇개의 클러스터를 구성할지 사전에 알 수 있을 때 사용하면 유용
안 좋은 상황 : 데이터가 비선형일 때, 군집 크기가 다를 때, 군집마다 밀집도와 거리가 다를 때
```

```
밀집 기반 군집 분석
왜 사용? : 주어진 데이터에 대한 군집화
언제 사용하면 좋을까? : K-평균 군집화와 다르게 사전에 클러스터의 숫자를 알지 못할 때 사용하면 유용
                      또한, 주어진 데이터에 이상치가 많이 포함되었을 때 사용하면 좋음
노이즈는 주어진 데이터셋과 무관하거나 무작위성 데이터로 전치리 과정에서 제거해야 할 부분
이상치란 관측된 데이터 범위에서 많이 벗어난 아주 작은 값이나 아주 큰 값

```

```
주성분 분석(PCA)
왜 사용? : 주어진 데이터의 간소화
언제 사용하면 좋을까? : 현재 데이터의 특성(변수)이 너무 많은 경우에는 데이터를 하나의 플롯(plot)에 시각화해서 살펴보는 것이 어렵다.
                      이때 특성 p개를 두세 개 정도로 압축하여 데이터를 시각화하여 살펴보고 싶을 때 유용한 알고리즘
변수가 많은 고차원 데이터의 경우 중요하지 않은 변수로 처리해야 할 데이터 양 많아지고 성능 또한 나빠지는 경향이 있다.
이러한 문제를 해결하고자 고차원 데이터를 저차원으로 축소시켜 데이터가 가진 대표 특성만 추출한다면
성능은 좋아지고 작업도 좀 더 간편해짐
대표적인 알고리즘이 PCA -> 고차원 데이터를 저차원 데이터로 축소시키는 알고리즘
```

Chapter 4
```
인공 신경망의 한계 -> AND, OR 는 선형 분류가 잘 되어 학습할 수 있지만,
XOR는 선형 분류를 할 수 없음 -> 해결방안은 중간에 은닉층을 둔 다층 퍼셉트론 사용
입력층과 출력층 사이에 여러 개의 신경망을 심층 신경망(DNN)이라고 하며 심층 신경망을 다른 말로 딥러닝이라고 함
```
```
딥러닝의 층은 입력층, 은닉층, 출력층으로 구성
가중치 : 노드와 노드 간 연결 강도
바이어스 : 가중치에 더해 주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값 조절
가중합 : 가중치와 신호의 곱을 합한 것
활성화 함수 : 신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수
손실 함수 : 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수
```
```
활성화 함수
전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수
시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 있음

시그모이드 함수
선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해 준다.
주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용된다.
하지만 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 '기울기 소멸 문제'가 발생하여 딥러닝에서 잘 사용X

하이퍼볼릭 탄젠트 함수
선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형해 준다.
시그모이드에서 결과 값 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만,
기울기 소멸 문제가 여전히 발생

렐루 함수
입력(x)이 음수일 때는 0을 양수일 때는 x를 출력
경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점이 있음
렐루 함수는 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수보다 6배 빠름
문제는 음수 값을 받으면 항상 0을 출력하기 때문에 학습 능력이 감소하는데,
이를 해결하려고 리키 렐루 함수 등을 사용

리키 렐루 함수
입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환
이렇게 하면 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제 해결

소프트맥스 함수
입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 한다.
소프트맥스 함수는 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용된다.
exp(x)는 지수함수이다. n은 출력층의 뉴런 개수, yk는 그중 k번째 출력을 의미
```
```
손실 함수
경사 하강법은 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트하는 방법이다.
즉, 미분의 기울기를 이용해 오차를 비교하고 최소화하는 방향으로 이동시키는 방법
이때 오차를 구하는 방법이 손실 함수

즉, 손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표
이 값이 클수록 많이 틀렸다는 의미고 0에 가까울수록 완벽하다는 것
대표적인 손실함수로는 평균 제곱 오차(MSE)와 크로스 엔트로피 오차(CEE)가 있다.

평균 제곱 오차
실제 값과 예측 값의 차이를 제곱하여 평균을 낸 것

크로스 엔트로피 오차
분류 문제에서 원-핫 인코딩 했을때만 사용할 수 있는 오차 계산법
일반적으로 분류 문제에서 데이터 출력을 0~1로 구분하기 위해 시그모이드 함수를 사용하는데,
시그모이드 함수에 포함된 자연 상수 e 때문에 평균 제곱 오차를 적용하면 매끄럽지 못한
그래프(울퉁불퉁한 그래프)가 출력된다. 따라서 크로스 엔트로피 손실 함수를 사용하는데,
이를 적용시 경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있다. 이것을 방지하고자
자연 상수 e에 반대되는 자연 로그를 모델의 출력 값에 취한다.
```
```
딥러닝 학습 - 순전파, 역전파
순전파는 훈련 데이터가 들어올 때 발생
예측 값은 최종 층(출력층)에 도달하게 됨
손실함수로 네트워크 예측 값과 실제 값의 차이(손실, 오차)를 추정하는데 이때 손실 함수 비용은 0이 이상적
따라서 손실 함수 비용이 0에 가깝도록 하기 위해 무델이 훈련을 반복하면서 가중치를 조정
손실(오차)이 계산되면 그 정보는 역으로 전파(출력층 -> 은닉층 -> 입력층)되기 때문에 역전파라고 함
```
```
과적합
과적합은 훈련 데이터를 과하게 학습해서 발생
훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상

기울기 소멸 문제는 은닉층이 많은 신경망에서 주로 발생

기울기 소멸 발생시 오차가 줄어들면서 학습이 되지 않으므로, 성능이 나빠지는 문제 발생
이를 개선하고자 확률적 경사 하강법과 미니 배치 경사 하강법을 사용

배치 경사 하강법(BGD)은 전체 데이터셋에 대한 오류를 구한 뒤 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트하는 방법
즉, 전체 훈련 데이터셋에 대해 가중치를 편미분하는 방법
배치 경사 하강법은 한 스텝에 모든 훈련 데이터셋을 사용하므로 학습이 오래 걸리는 단점이 있다.
오래걸리는 단점을 개선한 것이 확률적 경사 하강법

확률적 경사 하강법(SGD)은 임의로 선택한 데이터에 대해 기울기를 계산하는 방법으로 적은 데이터를 사용하므로 빠른 계산이 가능
때로는 배치 경사 하강법보다 정확도가 낮을 수 있지만 속도가 빠르다는 장점

미니 배치 경사 하강법은 전체 데이터셋을 미니 배치 여러개로 나누고, 미니 배치 한 개마다 기울기를 구한 후
그것을 평균 기울기를 이용해 모델을 업데이트해서 학습하는 방법
전체 데이터를 계산하는 것보다 빠르며, 확률적 경사 하강법보다 안정적이라는 장점이 있기에 많이 사용
안정적이며 속도 빠름

옵티마이저
확률적 경사 하강법의 파라미터 변경 폭이 불안정한 문제를 해결하기 위해 학습 속도와 운동량을 조정하는 옵티마이저를 적용해 볼 수 있다.
```
```
딥러닝 사용할 때 이점
특성 추출
데이터별로 어떤 특징을 가지고 있는지 찾아내고, 그것을 토대로 데이터를 벡터로 변환하는 작업을 특성 추출이라고 함
딥러닝에서는 특성 추출 과정을 알고리즘에 통합시킴. 데이터 특성을 잘 잡아내고자 은닉층을 깊게 쌓는 방식으로 파라미터를 늘린 모델 구조 덕분

빅데이터의 효율적 사용
딥러닝에서 특성추출을 알고리즘에 통합시키는 것이 가능한 이유는 빅데이터 때문
딥러닝 학습을 이용한 특성 추출은 데이터 사례가 많을수록 성능이 향상되기 때문

확보된 데이터가 적다면 딥러닝의 성능 향상을 기대하기 힘들기 때문에 머신러닝을 고려해 보아야 함
```
