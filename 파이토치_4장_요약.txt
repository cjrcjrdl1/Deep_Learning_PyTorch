인공 신경망을 이용하는 퍼셉트론의 한계
단층 퍼셉트론은 AND, OR는 학습이 가능했으나
XOR는 학습할 수 없다는 문제가 생김
-> 입력층과 출력층 사이 은닉층을 두어 비선형 문제 해결

가중치 : 입력 값의 연산 결과를 조정하는 역할
가중합 : 가중치 * 입력값 들의 합
이것들을 활성화 함수로 보내기 때문에
전달함수라고도 함
활성화 함수 :일정 기준에 따라 출력 값을 변화시키는 비선형 함수
- 시그모이드 함수
0~1 사이의 비선형 형태로 변형, 로지스틱 회귀 같은 문제를 확률적으로 표현하는 데 사용
but 기울기 소멸 문제로 잘 사용x
-하이퍼볼릭 탄젠트 함수
-1~1 사이에서 비선형 형태로 변형
시그모이드의 양수로 편향된 문제를 해결하였으나
기울기 소멸 문제는 여전히 발생
-렐루 함수
음수일때는 0 양수일 때는 값 그대로 출력
경사하강법에 영향 주지 않아 학습속도 빠르고 기울기 소멸 발생 x
음수 값을 입력받으면 항상 0을 출력하기 때문에 학습능력이 감소하는데 이를 해결하기 위해 리키 렐루함수 사용
-리키 렐루 함수
입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환
-소프트맥스 함수
0~1 사이에 출력되도록 정규화하여 총합이 1이 되도록 함
보통 출력 노드의 활성화 함수로 많이 사용

손실함수 : 학습률과 손실함수의 순간 기울기를 이용하여 가중치를 업데이트
미분의 기울기를 이용해 오차를 비교하고 최소화
0에 가까울수록 완벽

-평균 제곱 오차(MSE)
실제 값과 예측 값의 차이를 제곱하여 평균을 낸 것
이 값이 작을수록 예측력이 좋고
이는 회귀에서 손실함수로 주로 사용

-크로스 엔트로피 오차(CEE)
분류(classification) 문제에서 원-핫 레코드 했을때만 사용할 수 있는 오차 계산법
시그모이드는 MSE 적용시 매끄럽지 X
이 손실함수를 적용할 경우 경사 하강법과정에서
학습이 local minimum에서 멈출 수 있음
이를 방지하고자 자연 상수 e에 반대되는 자연로그를 모델 출력 값에 취함

딥러닝 - 순전파와 역전파

은닉층이 너무 많으면
오버피팅의 문제

오버피팅 방지하기 위해 드롭아웃 사용
일부 노드를 학습에서 제외

기울기 소멸 문제 발생
기울기가 0에 가까워지다 오차를 더 줄이지 못하고 그 상태로 수렴하는 현상
시그모이드 대신 렐루 사용하기

경사하강법 도중 성능 나빠지는 문제 발생

해결법

배치 경사 하강법(BGD)
전체 데이터셋에 대한 오류를 구한 뒤 기울기를 한번만 계산하여 모델 파라미터 업데이트
학습 오래걸리는 단점이 있음

확률적 경사 하강법(SGD)
임의로 선택한 데이터에 대해 기울기를 계산하는 방법으로 빠른 계산 가능
이는 파라미터 변경 폭이 불안정하고 BGD보다 정확도가 낮을 순 있지만 속도가 빠름

미니 배치 경사 하강법(mini-batch gradient descent)
전체 데이터셋을 미니 배치 여러개로 나누고
미니 배치 하나마다 기울기 구하고
그것의 평균 기울기를 이용해 모델 업데이트
이는 앞의 2개의 단점을 보완

옵티마이저
SGD의 파라미터 변경 폭이 불안정한 문제 해결을 위해
학습속도와 운동량을 조절하는 옵티마이저 적용 가능

속도를 조정하는 방법
아다그라드
변수(가중치)의 업데이트 횟수에 따라 학습률을 조정하는 방법
많이 변화하지 X 것은 학습률을 크게,
많이 변화하는 변수는 학습률 작게

아다델타
아다그라드에서 G(파라미터마다 다른 학습률을 주기 위한 함수) 값이 커짐에 따라 학습이 멈추는 문제를 해결하기 위해 등장한 방법

알엠에스프롭
아다그라드의 G(i) 값이 무한히 커지는 것을 방지하고자 제안된 방법

운동량을 조정하는 방법
모멘텀
경사하강법과 같이 매번 기울기를 구하지만 가중치 수정 전 이전 수정 방향(+,-)을 참고하여 같은 방향으로 일정한 비율만 수정하는 방법
모멘텀은 SGD와 함께 사용

네스테로프 모멘텀
기존 모멘텀과 달리 모멘텀 값이 적용된 지점에서 기울기 값 계산
모멘텀의 단점은 멈춰야 할 때도 관성에 의해 훨씬 멀리가는 단점이 있지만 네스테로프 모멘텀을 통해 단점 극복

속도와 운동량에 대한 혼용 방법
아담
모멘텀과 알엠에스프롭 장점을 결합한 경사하강법

특성 추출 : 데이터별로 어떤 특징을 가지고 있는지 찾아내고, 그것을 토대로 데이털르 벡터로 변환하는 작업을 특성 추출

딥러닝에서는 특성 추출 과정을 알고리즘에 통합(이것이 가능한 이유는 빅데이터 덕분)

딥러닝 알고리즘
딥러닝 알고리즘은 심층 신경망을 사용하는 공통점이 있음

심층 신경망(DNN)
심층 신경망은 입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공신경망
학습을 위한 연산량이 많으므로 기울기 소멸 문제가 발생할 수 있으나 이를 해결하기 위해 드롭아웃, 렐루 함수, 배치 정규화를 적용시킴

합성 신경망(CNN)
합성곱층과 풀링층을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘
영상이나 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치 찾아내는데 유용한 신경망

합성 신경망과 기존 신경망과 차이
각 층의 입출력 형상 유지
이미지 공간 정보 유지하며 인접 이미지와 차이가 있는 특징을 효과적으로 인식
복수 필터로 이미지의 특징을 추출하고 학습
추출한 이미지의 특징을 모으고 강화하는 풀링층이 있음
필터를 공유 파라미터로 사용하기 때문에 일반 인공 신경망과 비교하여 학습 파라미터가 매우 적음

순환신경망(RNN)
시계열 데이터(음악, 영상 등)같은 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망
순환신경망의 순환은 자기 자신을 참조한다는 것으로 현재결과가 이전 결과와 연관이 있다는 의미

순환신경망 특징
시간성을 가진 데이터가 많음
시간성 정보를 이용하여 데이터의 특징 잘 다룸
시간에 따라 내용이 변하므로 데이터는 동적이고 길이가 가변적
매우 긴 데이터르 처리하는 연구가 활발히 진행되고 있음
순환신경망은 기울기 소멸 문제로 학습이 제대로 되지 않는 문제가 있음
이를 해결하기 위해 메모리 개념 도입한 LSTM이 순환 신경망에서 많이 사용됨
순환신경망은 자연어 처리 분야와 궁합 맞음

제한된 볼츠만 머신(RBM)
볼츠만 머신은 가시층과 은닉층으로 구성
이 모델에선 가시층은 은닉층과만 연결되는데 이것은 제한된 볼츠만 머신

제한된 볼츠만 머신 특징
차원 감소, 분류, 선형회귀 분석, 협업 필터링,
특성 값 학습, 주제 모델링에 사용
기울기 소멸 문제를 해결하기 위해 사전 학습 용도로 활용 가능
심층 신뢰 신경망의 요소로 사용

심층 신뢰 신경망(DBN)
입력층과 은닉층으로 구성된 제한된 볼츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망
레이블이 없는 데이터에 대한 비지도 학습이 가능
부분적인 이미지에서 전체를 연상하는 일반화와 추상적 과정을 구현할 때 사용하면 유용

심층 신뢰 신경망 특징
순차적으로 심층 신뢰 신경망을 학습시켜 가면서 계층적 구조를 생성
비지도 학습으로 학습
위로 올라갈수록 추상적 특성을 추출
학습된 가중치를 다층 퍼셉트론의 가중치 초깃값으로 사용









